{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d27b5dd-39d4-4cd2-966e-f8997b59406b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 500"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*range() is a predefined method in spark to intiate 1000 rows with colum name as *Sequence_Number is column Name \n",
    "*it Basically creates the data frame with 1000 rows\n",
    "1)A DataFrame is the most common Structured API and simply represents a table of data with rows and columns.\n",
    "\"\"\"\n",
    "myRange = spark.range(1000).toDF(\"Sequence_Number\")\n",
    "#myRange.show();\n",
    "divsibleBy2=myRange.where(\"Sequence_Number%2=0\");\n",
    "#above line creates Data Frame even numbers in MyRange DataFrame\n",
    "#WHERE IS METHOD CALL ON DATA FRAME\n",
    "divsibleBy2.show();\n",
    "divsibleBy2.count();\n",
    "#count is Actions function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d71c983-1d27-46fe-b362-7078f1be7038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#332 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#332 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=592]\n      +- FileScan csv [DEST_COUNTRY_NAME#330,ORIGIN_COUNTRY_NAME#331,count#332] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "# spark Reading csv file\n",
    "#1).option(\"inferSchema\", \"true\"):  Spark will try to automatically determine the data types #of columns.\n",
    "#2)spark=It's the SparkSession object that is automatically available when you run a Spark #application.\n",
    "#3)myCsvData=spark\\.read\\.option(\"infraSchema\",\"true\")\\.option(\"header\",\"true\")\\.csv(\"dbfs:/#FileStore/2015_summary.csv\");\n",
    "# indentation is main for any pySpark programe\n",
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"dbfs:/FileStore/2015_summary.csv\")\n",
    "flightData2015.show();\n",
    "flightData2015.sort(\"count\").explain()\n",
    "#COUNTS NUMBER OF ROWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d648c8ef-ca55-4fc0-8806-f25cd6e7e89a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: [Row(max(count)=370002)]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bigdata_Learning_Day-1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
